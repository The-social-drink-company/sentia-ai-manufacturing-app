"""Add data import system tables

Revision ID: 9b7af71df6ab
Revises: 8f7b6d65ac0d
Create Date: 2025-08-26 17:59:20.190995

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = '9b7af71df6ab'
down_revision = '8f7b6d65ac0d'
branch_labels = None
depends_on = None


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('data_imports',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('import_name', sa.String(length=200), nullable=False),
    sa.Column('import_type', sa.Enum('PRODUCTS', 'HISTORICAL_SALES', 'INVENTORY_LEVELS', 'MANUFACTURING_DATA', 'FINANCIAL_DATA', 'FORECASTS', name='importtype'), nullable=False),
    sa.Column('import_description', sa.Text(), nullable=True),
    sa.Column('original_filename', sa.String(length=255), nullable=True),
    sa.Column('file_type', sa.Enum('CSV', 'XLSX', 'JSON', 'XML', 'API', name='filetype'), nullable=False),
    sa.Column('file_path', sa.String(length=500), nullable=True),
    sa.Column('file_size_bytes', sa.BigInteger(), nullable=True),
    sa.Column('file_hash', sa.String(length=64), nullable=True),
    sa.Column('status', sa.Enum('PENDING', 'PROCESSING', 'VALIDATING', 'COMPLETED', 'FAILED', 'CANCELLED', name='importstatus'), nullable=False),
    sa.Column('progress_percentage', sa.Integer(), nullable=True),
    sa.Column('current_step', sa.String(length=100), nullable=True),
    sa.Column('total_rows', sa.Integer(), nullable=True),
    sa.Column('processed_rows', sa.Integer(), nullable=True),
    sa.Column('successful_rows', sa.Integer(), nullable=True),
    sa.Column('failed_rows', sa.Integer(), nullable=True),
    sa.Column('duplicate_rows', sa.Integer(), nullable=True),
    sa.Column('data_quality_score', sa.Numeric(precision=3, scale=2), nullable=True),
    sa.Column('completeness_score', sa.Numeric(precision=3, scale=2), nullable=True),
    sa.Column('accuracy_score', sa.Numeric(precision=3, scale=2), nullable=True),
    sa.Column('import_settings', postgresql.JSON(astext_type=sa.Text()), nullable=True),
    sa.Column('validation_rules', postgresql.JSON(astext_type=sa.Text()), nullable=True),
    sa.Column('field_mappings', postgresql.JSON(astext_type=sa.Text()), nullable=True),
    sa.Column('started_at', sa.DateTime(timezone=True), nullable=True),
    sa.Column('completed_at', sa.DateTime(timezone=True), nullable=True),
    sa.Column('processing_duration_seconds', sa.Integer(), nullable=True),
    sa.Column('error_message', sa.Text(), nullable=True),
    sa.Column('error_details', postgresql.JSON(astext_type=sa.Text()), nullable=True),
    sa.Column('rollback_completed', sa.Boolean(), nullable=True),
    sa.Column('created_by', sa.UUID(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['created_by'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('data_imports', schema=None) as batch_op:
        batch_op.create_index('ix_data_imports_created_user', ['created_at', 'created_by'], unique=False)
        batch_op.create_index(batch_op.f('ix_data_imports_import_type'), ['import_type'], unique=False)
        batch_op.create_index('ix_data_imports_processing', ['status', 'started_at'], unique=False)
        batch_op.create_index(batch_op.f('ix_data_imports_status'), ['status'], unique=False)
        batch_op.create_index('ix_data_imports_status_type', ['status', 'import_type'], unique=False)

    op.create_table('import_templates',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('template_name', sa.String(length=100), nullable=False),
    sa.Column('import_type', sa.Enum('PRODUCTS', 'HISTORICAL_SALES', 'INVENTORY_LEVELS', 'MANUFACTURING_DATA', 'FINANCIAL_DATA', 'FORECASTS', name='importtype'), nullable=False),
    sa.Column('version', sa.String(length=20), nullable=True),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('file_format', sa.Enum('CSV', 'XLSX', 'JSON', 'XML', 'API', name='filetype'), nullable=False),
    sa.Column('field_definitions', postgresql.JSON(astext_type=sa.Text()), nullable=False),
    sa.Column('sample_data', postgresql.JSON(astext_type=sa.Text()), nullable=True),
    sa.Column('validation_rules', postgresql.JSON(astext_type=sa.Text()), nullable=True),
    sa.Column('template_file_path', sa.String(length=500), nullable=True),
    sa.Column('documentation_path', sa.String(length=500), nullable=True),
    sa.Column('download_count', sa.Integer(), nullable=True),
    sa.Column('usage_count', sa.Integer(), nullable=True),
    sa.Column('success_rate', sa.Numeric(precision=5, scale=2), nullable=True),
    sa.Column('is_active', sa.Boolean(), nullable=True),
    sa.Column('is_system_template', sa.Boolean(), nullable=True),
    sa.Column('created_by', sa.UUID(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['created_by'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('template_name')
    )
    with op.batch_alter_table('import_templates', schema=None) as batch_op:
        batch_op.create_index('ix_import_templates_type_active', ['import_type', 'is_active'], unique=False)
        batch_op.create_index('ix_import_templates_usage', ['usage_count', 'success_rate'], unique=False)

    op.create_table('import_errors',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('import_id', sa.UUID(), nullable=False),
    sa.Column('row_number', sa.Integer(), nullable=True),
    sa.Column('column_name', sa.String(length=100), nullable=True),
    sa.Column('error_type', sa.String(length=50), nullable=False),
    sa.Column('error_code', sa.String(length=20), nullable=True),
    sa.Column('error_message', sa.Text(), nullable=False),
    sa.Column('error_severity', sa.String(length=20), nullable=True),
    sa.Column('original_value', sa.Text(), nullable=True),
    sa.Column('suggested_value', sa.Text(), nullable=True),
    sa.Column('row_data', postgresql.JSON(astext_type=sa.Text()), nullable=True),
    sa.Column('is_resolved', sa.Boolean(), nullable=True),
    sa.Column('resolution_method', sa.String(length=50), nullable=True),
    sa.Column('resolved_by', sa.UUID(), nullable=True),
    sa.Column('resolved_at', sa.DateTime(timezone=True), nullable=True),
    sa.Column('resolution_notes', sa.Text(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['import_id'], ['data_imports.id'], ),
    sa.ForeignKeyConstraint(['resolved_by'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('import_errors', schema=None) as batch_op:
        batch_op.create_index('ix_import_errors_import_severity', ['import_id', 'error_severity'], unique=False)
        batch_op.create_index('ix_import_errors_row', ['import_id', 'row_number'], unique=False)
        batch_op.create_index('ix_import_errors_type_resolved', ['error_type', 'is_resolved'], unique=False)

    op.create_table('import_logs',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('import_id', sa.UUID(), nullable=False),
    sa.Column('log_level', sa.String(length=20), nullable=False),
    sa.Column('log_message', sa.Text(), nullable=False),
    sa.Column('log_context', postgresql.JSON(astext_type=sa.Text()), nullable=True),
    sa.Column('step_name', sa.String(length=100), nullable=True),
    sa.Column('row_number', sa.Integer(), nullable=True),
    sa.Column('processing_time_ms', sa.Integer(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['import_id'], ['data_imports.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('import_logs', schema=None) as batch_op:
        batch_op.create_index('ix_import_logs_created', ['created_at'], unique=False)
        batch_op.create_index('ix_import_logs_import_level', ['import_id', 'log_level'], unique=False)

    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('import_logs', schema=None) as batch_op:
        batch_op.drop_index('ix_import_logs_import_level')
        batch_op.drop_index('ix_import_logs_created')

    op.drop_table('import_logs')
    with op.batch_alter_table('import_errors', schema=None) as batch_op:
        batch_op.drop_index('ix_import_errors_type_resolved')
        batch_op.drop_index('ix_import_errors_row')
        batch_op.drop_index('ix_import_errors_import_severity')

    op.drop_table('import_errors')
    with op.batch_alter_table('import_templates', schema=None) as batch_op:
        batch_op.drop_index('ix_import_templates_usage')
        batch_op.drop_index('ix_import_templates_type_active')

    op.drop_table('import_templates')
    with op.batch_alter_table('data_imports', schema=None) as batch_op:
        batch_op.drop_index('ix_data_imports_status_type')
        batch_op.drop_index(batch_op.f('ix_data_imports_status'))
        batch_op.drop_index('ix_data_imports_processing')
        batch_op.drop_index(batch_op.f('ix_data_imports_import_type'))
        batch_op.drop_index('ix_data_imports_created_user')

    op.drop_table('data_imports')
    # ### end Alembic commands ###
