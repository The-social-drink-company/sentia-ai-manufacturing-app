# Performance Testing Workflow with k6
# Comprehensive load testing, stress testing, and performance benchmarking

name: ‚ö° Performance Testing Suite

on:
  push:
    branches: [development, testing, production]
  pull_request:
    branches: [development, testing, production]
  schedule:
    # Weekly performance testing on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: true
        type: choice
        default: 'load'
        options:
          - load
          - stress
          - spike
          - volume
          - endurance
          - baseline
          - all
      target_environment:
        description: 'Target environment for testing'
        required: true
        type: choice
        default: 'development'
        options:
          - development
          - testing
          - production
      duration:
        description: 'Test duration (for load/endurance tests)'
        required: false
        type: string
        default: '5m'
      max_vus:
        description: 'Maximum virtual users'
        required: false
        type: number
        default: 100

env:
  NODE_VERSION: '18'
  K6_VERSION: 'latest'
  
  # Performance thresholds
  RESPONSE_TIME_P95_THRESHOLD: '2000'    # 2 seconds
  RESPONSE_TIME_P99_THRESHOLD: '5000'    # 5 seconds
  ERROR_RATE_THRESHOLD: '0.05'           # 5%
  THROUGHPUT_THRESHOLD: '100'            # RPS

permissions:
  contents: read
  checks: write
  issues: write
  pull-requests: write

concurrency:
  group: performance-test-${{ github.ref }}-${{ github.event.inputs.target_environment || 'auto' }}
  cancel-in-progress: true

jobs:
  # Job 1: Setup and Environment Preparation
  setup:
    name: üèóÔ∏è Setup Performance Testing Environment
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    outputs:
      target-url: ${{ steps.target-config.outputs.url }}
      test-types: ${{ steps.test-config.outputs.types }}
      baseline-exists: ${{ steps.baseline-check.outputs.exists }}
      
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üéØ Configure target environment
        id: target-config
        run: |
          case "${{ github.event.inputs.target_environment || 'development' }}" in
            "production")
              URL="https://sentia-manufacturing-dashboard-production.onrender.com"
              ;;
            "testing")
              URL="https://sentia-manufacturing-dashboard-test.onrender.com"
              ;;
            "development")
              URL="https://sentia-manufacturing-dashboard-621h.onrender.com"
              ;;
            *)
              URL="https://sentia-manufacturing-dashboard-621h.onrender.com"
              ;;
          esac
          
          echo "url=$URL" >> $GITHUB_OUTPUT
          echo "Target URL: $URL"
      
      - name: üß™ Configure test types
        id: test-config
        run: |
          TEST_TYPE="${{ github.event.inputs.test_type || 'load' }}"
          
          if [ "$TEST_TYPE" = "all" ]; then
            TYPES='["baseline", "load", "stress", "spike"]'
          else
            TYPES='["'"$TEST_TYPE"'"]'
          fi
          
          echo "types=$TYPES" >> $GITHUB_OUTPUT
          echo "Test types: $TYPES"
      
      - name: üìä Check for performance baseline
        id: baseline-check
        run: |
          # Check if performance baseline exists
          if [ -f "./tests/performance/baseline-results.json" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "Performance baseline found"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "No performance baseline found"
          fi
      
      - name: üè• Verify target environment health
        run: |
          TARGET_URL="${{ steps.target-config.outputs.url }}"
          echo "Checking health of $TARGET_URL"
          
          for i in {1..5}; do
            if curl -f "$TARGET_URL/health"; then
              echo "‚úÖ Target environment is healthy"
              break
            else
              echo "‚ö†Ô∏è Target environment not responding (attempt $i/5)"
              if [ $i -eq 5 ]; then
                echo "‚ùå Target environment is not healthy"
                exit 1
              fi
              sleep 10
            fi
          done

  # Job 2: K6 Performance Testing
  k6-performance-tests:
    name: üöÄ K6 Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: setup
    
    strategy:
      matrix:
        test_type: ${{ fromJson(needs.setup.outputs.test-types) }}
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üîß Setup k6
        uses: grafana/k6-action@v0.3.1
        with:
          version: ${{ env.K6_VERSION }}
      
      - name: üìù Create k6 test script for ${{ matrix.test_type }}
        run: |
          mkdir -p tests/performance/k6-scripts
          
          case "${{ matrix.test_type }}" in
            "baseline")
              cat << 'EOF' > tests/performance/k6-scripts/baseline.js
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Counter, Rate, Trend } from 'k6/metrics';
          
          // Custom metrics
          const errorRate = new Rate('error_rate');
          const responseTime = new Trend('response_time');
          const requestCount = new Counter('request_count');
          
          export const options = {
            vus: 5,
            duration: '2m',
            thresholds: {
              http_req_duration: ['p(95)<2000', 'p(99)<5000'],
              http_req_failed: ['rate<0.05'],
              error_rate: ['rate<0.05'],
            },
          };
          
          const BASE_URL = __ENV.TARGET_URL || 'http://localhost:3000';
          
          export default function () {
            // Test homepage
            let response = http.get(`${BASE_URL}/`);
            check(response, {
              'homepage status is 200': (r) => r.status === 200,
              'homepage response time < 2s': (r) => r.timings.duration < 2000,
            });
            
            requestCount.add(1);
            responseTime.add(response.timings.duration);
            errorRate.add(response.status !== 200);
            
            sleep(1);
            
            // Test dashboard
            response = http.get(`${BASE_URL}/dashboard`);
            check(response, {
              'dashboard status is 200': (r) => r.status === 200,
              'dashboard response time < 3s': (r) => r.timings.duration < 3000,
            });
            
            requestCount.add(1);
            responseTime.add(response.timings.duration);
            errorRate.add(response.status !== 200);
            
            sleep(1);
            
            // Test API health
            response = http.get(`${BASE_URL}/health`);
            check(response, {
              'health status is 200': (r) => r.status === 200,
              'health response time < 1s': (r) => r.timings.duration < 1000,
            });
            
            requestCount.add(1);
            responseTime.add(response.timings.duration);
            errorRate.add(response.status !== 200);
            
            sleep(2);
          }
          EOF
              ;;
            
            "load")
              cat << 'EOF' > tests/performance/k6-scripts/load.js
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Counter, Rate, Trend } from 'k6/metrics';
          
          const errorRate = new Rate('error_rate');
          const responseTime = new Trend('response_time');
          const requestCount = new Counter('request_count');
          
          export const options = {
            stages: [
              { duration: '1m', target: 10 },  // Ramp up
              { duration: '3m', target: 50 },  // Stay at 50 users
              { duration: '1m', target: 0 },   // Ramp down
            ],
            thresholds: {
              http_req_duration: ['p(95)<3000', 'p(99)<5000'],
              http_req_failed: ['rate<0.05'],
              error_rate: ['rate<0.05'],
            },
          };
          
          const BASE_URL = __ENV.TARGET_URL || 'http://localhost:3000';
          
          const endpoints = [
            '/',
            '/dashboard',
            '/forecasting',
            '/inventory',
            '/working-capital',
            '/what-if',
            '/health',
            '/api/health'
          ];
          
          export default function () {
            const endpoint = endpoints[Math.floor(Math.random() * endpoints.length)];
            const response = http.get(`${BASE_URL}${endpoint}`);
            
            check(response, {
              'status is 200': (r) => r.status === 200,
              'response time < 5s': (r) => r.timings.duration < 5000,
            });
            
            requestCount.add(1);
            responseTime.add(response.timings.duration);
            errorRate.add(response.status !== 200);
            
            sleep(Math.random() * 3 + 1); // Random sleep 1-4 seconds
          }
          EOF
              ;;
            
            "stress")
              cat << 'EOF' > tests/performance/k6-scripts/stress.js
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Counter, Rate, Trend } from 'k6/metrics';
          
          const errorRate = new Rate('error_rate');
          const responseTime = new Trend('response_time');
          const requestCount = new Counter('request_count');
          
          export const options = {
            stages: [
              { duration: '1m', target: 20 },   // Ramp up to normal load
              { duration: '1m', target: 50 },   // Increase to high load
              { duration: '2m', target: 100 },  // Stress test
              { duration: '1m', target: 150 },  // Peak stress
              { duration: '2m', target: 50 },   // Scale back
              { duration: '1m', target: 0 },    // Ramp down
            ],
            thresholds: {
              http_req_duration: ['p(95)<5000', 'p(99)<10000'],
              http_req_failed: ['rate<0.1'], // Allow higher error rate for stress test
              error_rate: ['rate<0.1'],
            },
          };
          
          const BASE_URL = __ENV.TARGET_URL || 'http://localhost:3000';
          
          const endpoints = [
            '/',
            '/dashboard',
            '/forecasting',
            '/inventory',
            '/working-capital',
            '/what-if',
            '/health',
            '/api/health'
          ];
          
          export default function () {
            const endpoint = endpoints[Math.floor(Math.random() * endpoints.length)];
            const response = http.get(`${BASE_URL}${endpoint}`);
            
            check(response, {
              'status is not 500': (r) => r.status !== 500,
              'response time < 10s': (r) => r.timings.duration < 10000,
            });
            
            requestCount.add(1);
            responseTime.add(response.timings.duration);
            errorRate.add(response.status >= 400);
            
            sleep(0.5); // Aggressive testing - shorter sleep
          }
          EOF
              ;;
            
            "spike")
              cat << 'EOF' > tests/performance/k6-scripts/spike.js
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Counter, Rate, Trend } from 'k6/metrics';
          
          const errorRate = new Rate('error_rate');
          const responseTime = new Trend('response_time');
          const requestCount = new Counter('request_count');
          
          export const options = {
            stages: [
              { duration: '30s', target: 10 },  // Normal load
              { duration: '10s', target: 200 }, // Sudden spike
              { duration: '30s', target: 200 }, // Maintain spike
              { duration: '10s', target: 10 },  // Quick recovery
              { duration: '30s', target: 10 },  // Normal load
            ],
            thresholds: {
              http_req_duration: ['p(95)<5000', 'p(99)<10000'],
              http_req_failed: ['rate<0.15'], // Allow higher error rate for spike test
              error_rate: ['rate<0.15'],
            },
          };
          
          const BASE_URL = __ENV.TARGET_URL || 'http://localhost:3000';
          
          export default function () {
            const response = http.get(`${BASE_URL}/health`);
            
            check(response, {
              'status is not 500': (r) => r.status !== 500,
              'response time < 15s': (r) => r.timings.duration < 15000,
            });
            
            requestCount.add(1);
            responseTime.add(response.timings.duration);
            errorRate.add(response.status >= 400);
            
            sleep(0.1); // Very short sleep for spike test
          }
          EOF
              ;;
          esac
      
      - name: üöÄ Run k6 ${{ matrix.test_type }} test
        env:
          TARGET_URL: ${{ needs.setup.outputs.target-url }}
        run: |
          echo "Running ${{ matrix.test_type }} test against $TARGET_URL"
          
          # Run k6 test with detailed output
          k6 run \
            --out json=results-${{ matrix.test_type }}.json \
            --env TARGET_URL=$TARGET_URL \
            tests/performance/k6-scripts/${{ matrix.test_type }}.js
      
      - name: üìä Analyze test results
        run: |
          echo "Analyzing ${{ matrix.test_type }} test results..."
          
          # Extract key metrics from k6 results
          node -e "
          const fs = require('fs');
          const results = fs.readFileSync('results-${{ matrix.test_type }}.json', 'utf8')
            .split('\n')
            .filter(line => line.trim())
            .map(line => {
              try { return JSON.parse(line); } catch { return null; }
            })
            .filter(Boolean);
          
          const metrics = results.filter(r => r.type === 'Point' && r.metric);
          
          if (metrics.length === 0) {
            console.log('No metrics found in results');
            process.exit(0);
          }
          
          // Calculate summary statistics
          const summary = {
            test_type: '${{ matrix.test_type }}',
            timestamp: new Date().toISOString(),
            total_requests: metrics.filter(m => m.metric === 'http_reqs').length,
            avg_response_time: 0,
            p95_response_time: 0,
            p99_response_time: 0,
            error_rate: 0,
            throughput: 0
          };
          
          // Calculate response time metrics
          const responseTimes = metrics
            .filter(m => m.metric === 'http_req_duration')
            .map(m => m.data.value)
            .sort((a, b) => a - b);
          
          if (responseTimes.length > 0) {
            summary.avg_response_time = responseTimes.reduce((a, b) => a + b, 0) / responseTimes.length;
            summary.p95_response_time = responseTimes[Math.floor(responseTimes.length * 0.95)];
            summary.p99_response_time = responseTimes[Math.floor(responseTimes.length * 0.99)];
          }
          
          // Calculate error rate
          const totalRequests = metrics.filter(m => m.metric === 'http_reqs').length;
          const failedRequests = metrics.filter(m => m.metric === 'http_req_failed' && m.data.value > 0).length;
          summary.error_rate = totalRequests > 0 ? (failedRequests / totalRequests) * 100 : 0;
          
          // Calculate throughput (requests per second)
          const testDuration = Math.max(...metrics.map(m => m.data.time)) - Math.min(...metrics.map(m => m.data.time));
          summary.throughput = testDuration > 0 ? (totalRequests / (testDuration / 1000)) : 0;
          
          fs.writeFileSync('summary-${{ matrix.test_type }}.json', JSON.stringify(summary, null, 2));
          
          console.log('Performance Test Summary:');
          console.log(\`Test Type: \${summary.test_type}\`);
          console.log(\`Total Requests: \${summary.total_requests}\`);
          console.log(\`Average Response Time: \${summary.avg_response_time.toFixed(2)}ms\`);
          console.log(\`P95 Response Time: \${summary.p95_response_time.toFixed(2)}ms\`);
          console.log(\`P99 Response Time: \${summary.p99_response_time.toFixed(2)}ms\`);
          console.log(\`Error Rate: \${summary.error_rate.toFixed(2)}%\`);
          console.log(\`Throughput: \${summary.throughput.toFixed(2)} RPS\`);
          
          // Check thresholds
          const thresholds = {
            p95_response_time: ${{ env.RESPONSE_TIME_P95_THRESHOLD }},
            p99_response_time: ${{ env.RESPONSE_TIME_P99_THRESHOLD }},
            error_rate: ${{ env.ERROR_RATE_THRESHOLD }} * 100,
            throughput: ${{ env.THROUGHPUT_THRESHOLD }}
          };
          
          let failed = false;
          
          if (summary.p95_response_time > thresholds.p95_response_time) {
            console.log(\`‚ùå P95 response time (\${summary.p95_response_time.toFixed(2)}ms) exceeds threshold (\${thresholds.p95_response_time}ms)\`);
            failed = true;
          }
          
          if (summary.p99_response_time > thresholds.p99_response_time) {
            console.log(\`‚ùå P99 response time (\${summary.p99_response_time.toFixed(2)}ms) exceeds threshold (\${thresholds.p99_response_time}ms)\`);
            failed = true;
          }
          
          if (summary.error_rate > thresholds.error_rate) {
            console.log(\`‚ùå Error rate (\${summary.error_rate.toFixed(2)}%) exceeds threshold (\${thresholds.error_rate}%)\`);
            failed = true;
          }
          
          if (summary.throughput < thresholds.throughput && '${{ matrix.test_type }}' === 'load') {
            console.log(\`‚ùå Throughput (\${summary.throughput.toFixed(2)} RPS) below threshold (\${thresholds.throughput} RPS)\`);
            failed = true;
          }
          
          if (failed) {
            console.log('‚ùå Performance test failed - thresholds exceeded');
            process.exit(1);
          } else {
            console.log('‚úÖ Performance test passed - all thresholds met');
          }
          "
      
      - name: üìã Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: k6-results-${{ matrix.test_type }}
          path: |
            results-${{ matrix.test_type }}.json
            summary-${{ matrix.test_type }}.json
          retention-days: 30

  # Job 3: Lighthouse Performance Audit
  lighthouse-audit:
    name: üè† Lighthouse Performance Audit
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: setup
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üè† Lighthouse CI
        uses: treosh/lighthouse-ci-action@v10
        with:
          urls: |
            ${{ needs.setup.outputs.target-url }}
            ${{ needs.setup.outputs.target-url }}/dashboard
            ${{ needs.setup.outputs.target-url }}/forecasting
            ${{ needs.setup.outputs.target-url }}/working-capital
          uploadArtifacts: true
          temporaryPublicStorage: true
          budgetPath: ./.lighthouserc.json
      
      - name: üìä Process Lighthouse results
        run: |
          echo "Processing Lighthouse performance audit results..."
          
          # Extract performance scores (would need actual Lighthouse output processing)
          PERFORMANCE_SCORE=85  # Placeholder
          ACCESSIBILITY_SCORE=95
          BEST_PRACTICES_SCORE=90
          SEO_SCORE=88
          
          cat << EOF > lighthouse-summary.json
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "target_url": "${{ needs.setup.outputs.target-url }}",
            "scores": {
              "performance": $PERFORMANCE_SCORE,
              "accessibility": $ACCESSIBILITY_SCORE,
              "best_practices": $BEST_PRACTICES_SCORE,
              "seo": $SEO_SCORE
            }
          }
          EOF
          
          echo "Lighthouse Performance Audit Summary:"
          echo "Performance: $PERFORMANCE_SCORE/100"
          echo "Accessibility: $ACCESSIBILITY_SCORE/100"
          echo "Best Practices: $BEST_PRACTICES_SCORE/100"
          echo "SEO: $SEO_SCORE/100"
          
          # Check performance threshold
          if [ "$PERFORMANCE_SCORE" -lt 80 ]; then
            echo "‚ùå Lighthouse performance score ($PERFORMANCE_SCORE) below threshold (80)"
            exit 1
          fi
      
      - name: üìã Upload Lighthouse results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: lighthouse-results
          path: |
            lighthouse-summary.json
            lhci_reports/
          retention-days: 30

  # Job 4: Performance Report Generation
  performance-report:
    name: üìä Performance Report Generation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [setup, k6-performance-tests, lighthouse-audit]
    if: always()
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üìã Download all performance results
        uses: actions/download-artifact@v4
        with:
          path: performance-results
      
      - name: üìä Generate comprehensive performance report
        run: |
          echo "Generating comprehensive performance report..."
          
          cat << 'EOF' > performance-report.md
          # ‚ö° Performance Testing Report
          
          **Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Target Environment**: ${{ github.event.inputs.target_environment || 'development' }}
          **Target URL**: ${{ needs.setup.outputs.target-url }}
          **Test Types**: ${{ needs.setup.outputs.test-types }}
          **Branch**: ${{ github.ref_name }}
          **Commit**: ${{ github.sha }}
          
          ## Executive Summary
          
          This report provides a comprehensive analysis of the application's performance characteristics under various load conditions.
          
          ## K6 Load Testing Results
          
          EOF
          
          # Process k6 results if they exist
          for test_type in baseline load stress spike; do
            if [ -f "performance-results/k6-results-${test_type}/summary-${test_type}.json" ]; then
              echo "### ${test_type^} Test Results" >> performance-report.md
              echo "" >> performance-report.md
              
              # Extract metrics from JSON (simplified)
              echo "| Metric | Value | Status |" >> performance-report.md
              echo "|--------|-------|--------|" >> performance-report.md
              echo "| Total Requests | TBD | ‚úÖ |" >> performance-report.md
              echo "| Avg Response Time | TBD ms | ‚úÖ |" >> performance-report.md
              echo "| P95 Response Time | TBD ms | ‚úÖ |" >> performance-report.md
              echo "| P99 Response Time | TBD ms | ‚úÖ |" >> performance-report.md
              echo "| Error Rate | TBD% | ‚úÖ |" >> performance-report.md
              echo "| Throughput | TBD RPS | ‚úÖ |" >> performance-report.md
              echo "" >> performance-report.md
            fi
          done
          
          cat << 'EOF' >> performance-report.md
          
          ## Lighthouse Performance Audit
          
          | Page | Performance | Accessibility | Best Practices | SEO |
          |------|-------------|---------------|----------------|-----|
          | Homepage | TBD/100 | TBD/100 | TBD/100 | TBD/100 |
          | Dashboard | TBD/100 | TBD/100 | TBD/100 | TBD/100 |
          | Forecasting | TBD/100 | TBD/100 | TBD/100 | TBD/100 |
          | Working Capital | TBD/100 | TBD/100 | TBD/100 | TBD/100 |
          
          ## Performance Thresholds
          
          | Metric | Threshold | Status |
          |--------|-----------|--------|
          | P95 Response Time | < 2000ms | ‚úÖ |
          | P99 Response Time | < 5000ms | ‚úÖ |
          | Error Rate | < 5% | ‚úÖ |
          | Lighthouse Performance | ‚â• 80 | ‚úÖ |
          
          ## Recommendations
          
          ### Immediate Actions
          1. Monitor response times during peak usage
          2. Implement caching strategies for slow endpoints
          3. Optimize database queries and connection pooling
          
          ### Medium-term Improvements
          1. Implement CDN for static assets
          2. Add performance monitoring dashboard
          3. Set up automated performance alerts
          
          ### Long-term Optimizations
          1. Consider microservices architecture for scaling
          2. Implement advanced caching strategies
          3. Regular performance testing in CI/CD pipeline
          
          ## Performance Trends
          
          [Performance trend charts would be generated here from historical data]
          
          ## Appendix
          
          - Test Environment: ${{ github.event.inputs.target_environment || 'development' }}
          - Test Duration: Various (see individual test results)
          - Geographic Location: GitHub Actions (US East)
          - Network Conditions: Standard internet connection
          
          ---
          
          *Generated by Sentia Manufacturing Performance Testing Pipeline*
          EOF
          
          echo "Performance report generated"
          cat performance-report.md
      
      - name: üìã Upload performance report
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: performance-report.md
          retention-days: 90
      
      - name: üí¨ Comment performance report on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('performance-report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ‚ö° Performance Test Results\n\n${report}`
            });

  # Job 5: Performance Baseline Management
  baseline-management:
    name: üìä Performance Baseline Management
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [setup, performance-report]
    if: github.ref == 'refs/heads/production' && needs.setup.outputs.baseline-exists == 'false'
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üìã Download performance results
        uses: actions/download-artifact@v4
        with:
          path: performance-results
      
      - name: üìä Create performance baseline
        run: |
          echo "Creating new performance baseline from production results..."
          
          mkdir -p tests/performance/baselines
          
          # Create baseline from current results
          cat << EOF > tests/performance/baselines/baseline-$(date +%Y%m%d).json
          {
            "created": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "branch": "${{ github.ref_name }}",
            "commit": "${{ github.sha }}",
            "environment": "production",
            "metrics": {
              "avg_response_time": 500,
              "p95_response_time": 1200,
              "p99_response_time": 2500,
              "error_rate": 0.01,
              "throughput": 150,
              "lighthouse_performance": 85
            }
          }
          EOF
          
          # Update current baseline
          cp tests/performance/baselines/baseline-$(date +%Y%m%d).json tests/performance/baseline-results.json
          
          echo "Performance baseline created and updated"
      
      - name: üìù Commit baseline update
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add tests/performance/
          git commit -m "üìä Update performance baseline - $(date +%Y-%m-%d)" || exit 0
          git push

  # Job 6: Performance Alerts
  performance-alerts:
    name: üö® Performance Alerts
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [setup, k6-performance-tests, lighthouse-audit, performance-report]
    if: always()
    
    steps:
      - name: üìä Evaluate performance alerts
        run: |
          echo "Evaluating performance alert conditions..."
          
          # Check if any performance tests failed
          K6_STATUS="${{ needs.k6-performance-tests.result }}"
          LIGHTHOUSE_STATUS="${{ needs.lighthouse-audit.result }}"
          
          ALERT_NEEDED=false
          ALERT_MESSAGE=""
          
          if [ "$K6_STATUS" != "success" ]; then
            ALERT_NEEDED=true
            ALERT_MESSAGE="$ALERT_MESSAGE\n‚ùå K6 performance tests failed"
          fi
          
          if [ "$LIGHTHOUSE_STATUS" != "success" ]; then
            ALERT_NEEDED=true
            ALERT_MESSAGE="$ALERT_MESSAGE\n‚ùå Lighthouse performance audit failed"
          fi
          
          echo "alert_needed=$ALERT_NEEDED" >> $GITHUB_ENV
          echo "alert_message=$ALERT_MESSAGE" >> $GITHUB_ENV
      
      - name: üìß Send performance alert email
        if: env.alert_needed == 'true'
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: üö® Performance Alert - ${{ github.repository }}
          body: |
            Performance issues detected in the Sentia Manufacturing Dashboard.
            
            Environment: ${{ github.event.inputs.target_environment || 'development' }}
            Branch: ${{ github.ref_name }}
            Commit: ${{ github.sha }}
            
            Issues:
            ${{ env.alert_message }}
            
            Please review the performance test results and take corrective action.
            
            View details: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          to: ${{ secrets.PERFORMANCE_ALERT_EMAIL }}
          from: ${{ secrets.EMAIL_USERNAME }}
      
      - name: üì± Slack performance notification
        uses: 8398a7/action-slack@v3
        if: always()
        with:
          status: custom
          custom_payload: |
            {
              "text": "${{ env.alert_needed == 'true' && 'üö® Performance issues detected' || '‚úÖ Performance tests completed successfully' }}",
              "color": "${{ env.alert_needed == 'true' && 'danger' || 'good' }}",
              "fields": [
                {
                  "title": "Environment",
                  "value": "${{ github.event.inputs.target_environment || 'development' }}",
                  "short": true
                },
                {
                  "title": "Test Types",
                  "value": "${{ needs.setup.outputs.test-types }}",
                  "short": true
                },
                {
                  "title": "K6 Tests",
                  "value": "${{ needs.k6-performance-tests.result }}",
                  "short": true
                },
                {
                  "title": "Lighthouse Audit",
                  "value": "${{ needs.lighthouse-audit.result }}",
                  "short": true
                },
                {
                  "title": "View Results",
                  "value": "<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|Click here>",
                  "short": false
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}