/**
 * Enterprise Chaos Engineering Engine - Infrastructure Resilience Testing Framework
 * Implements fault injection, network partitioning, latency simulation,
 * and automated resilience validation for production-grade systems
 */

import fs from 'fs';
import path from 'path';
import { exec, spawn } from 'child_process';
import { promisify } from 'util';
import EventEmitter from 'events';
import os from 'os';
import { logDebug, logInfo, logWarn, logError } from '../../src/utils/logger';


const execAsync = promisify(exec);

class ChaosEngineeringEngine extends EventEmitter {
  constructor(config = {}) {
    super();
    
    this.config = {
      // Chaos experiment configuration
      experiments: {
        networkLatency: {
          enabled: true,
          minLatency: 100, // ms
          maxLatency: 5000, // ms
          duration: 60000, // 1 minute
          targetHosts: ['localhost', '127.0.0.1']
        },
        
        networkPartition: {
          enabled: true,
          duration: 30000, // 30 seconds
          targetPorts: [3000, 5000, 5432, 6379],
          recoveryTime: 10000 // 10 seconds
        },
        
        resourceExhaustion: {
          enabled: true,
          cpu: { enabled: true, targetPercentage: 90 },
          memory: { enabled: true, targetPercentage: 85 },
          disk: { enabled: true, targetPercentage: 90 },
          duration: 45000 // 45 seconds
        },
        
        serviceFailure: {
          enabled: true,
          targetServices: ['api-server', 'database', 'cache'],
          failureTypes: ['crash', 'hang', 'slow-response'],
          duration: 30000
        },
        
        databaseFailure: {
          enabled: true,
          connectionKill: true,
          queryLatency: true,
          transactionFailure: true,
          duration: 20000
        },
        
        fileSystemChaos: {
          enabled: true,
          diskFull: true,
          permissionErrors: true,
          ioErrors: true,
          duration: 30000
        }
      },
      
      // Safety and constraints
      safety: {
        maxConcurrentExperiments: 2,
        cooldownPeriod: 60000, // 1 minute between experiments
        maxExperimentDuration: 300000, // 5 minutes max
        emergencyStopTriggers: [
          'critical_service_down',
          'data_corruption_detected',
          'user_impact_threshold_exceeded'
        ],
        healthCheckInterval: 5000,
        autoRecovery: true
      },
      
      // Monitoring and observability
      monitoring: {
        metricsCollection: true,
        alerting: true,
        loggingLevel: 'INFO',
        dashboardUpdates: true,
        realTimeMetrics: [
          'response_time',
          'error_rate',
          'throughput',
          'resource_usage',
          'service_availability'
        ]
      },
      
      // Recovery mechanisms
      recovery: {
        automaticRollback: true,
        rollbackTimeout: 30000,
        healthValidation: true,
        serviceRestart: true,
        circuitBreakerReset: true
      },
      
      ...config
    };

    this.activeExperiments = new Map();
    this.experimentHistory = [];
    this.systemBaseline = null;
    this.emergencyStopActive = false;
    this.chaosTools = new Map();
    
    this.initialize();
  }

  async initialize() {
    logDebug('üí• INITIALIZING CHAOS ENGINEERING ENGINE');
    
    // Setup chaos experiment directories
    this.setupChaosDirectories();
    
    // Initialize chaos tools
    await this.initializeChaosTools();
    
    // Establish system baseline
    await this.establishSystemBaseline();
    
    // Setup monitoring
    await this.setupChaosMonitoring();
    
    logDebug('‚úÖ Chaos Engineering Engine initialized successfully');
    this.emit('initialized');
  }

  setupChaosDirectories() {
    const dirs = [
      'tests/chaos/experiments',
      'tests/chaos/reports',
      'tests/chaos/scripts',
      'tests/chaos/baselines',
      'logs/chaos-engineering'
    ];

    dirs.forEach(dir => {
      const fullPath = path.join(process.cwd(), dir);
      if (!fs.existsSync(fullPath)) {
        fs.mkdirSync(fullPath, { recursive: true });
      }
    });
  }

  async initializeChaosTools() {
    // Network chaos tools
    this.chaosTools.set('network', new NetworkChaosController());
    
    // Resource chaos tools
    this.chaosTools.set('resource', new ResourceChaosController());
    
    // Service chaos tools  
    this.chaosTools.set('service', new ServiceChaosController());
    
    // Database chaos tools
    this.chaosTools.set('database', new DatabaseChaosController());
    
    // File system chaos tools
    this.chaosTools.set('filesystem', new FileSystemChaosController());

    logDebug(`üõ†Ô∏è Initialized ${this.chaosTools.size} chaos tool controllers`);
  }

  async establishSystemBaseline() {
    logDebug('üìä Establishing system performance baseline...');
    
    const baseline = {
      timestamp: new Date().toISOString(),
      metrics: {
        responseTime: await this.measureResponseTime(),
        throughput: await this.measureThroughput(),
        errorRate: await this.measureErrorRate(),
        resourceUsage: await this.measureResourceUsage(),
        serviceHealth: await this.checkServiceHealth()
      }
    };

    this.systemBaseline = baseline;
    
    // Save baseline for historical tracking
    fs.writeFileSync(
      path.join(process.cwd(), 'tests/chaos/baselines/system-baseline.json'),
      JSON.stringify(baseline, null, 2)
    );

    logDebug('‚úÖ System baseline established');
    return baseline;
  }

  async setupChaosMonitoring() {
    // Setup real-time monitoring for chaos experiments
    this.monitoringInterval = setInterval(async _() => {
      if (this.activeExperiments.size > 0) {
        await this.collectChaosMetrics();
        await this.checkEmergencyConditions();
      }
    }, this.config.monitoring.healthCheckInterval);

    logDebug('üì° Chaos monitoring system activated');
  }

  // Main chaos experiment execution
  async runChaosExperiment(experimentType, customConfig = {}) {
    if (this.emergencyStopActive) {
      throw new Error('Emergency stop is active - chaos experiments disabled');
    }

    if (this.activeExperiments.size >= this.config.safety.maxConcurrentExperiments) {
      throw new Error('Maximum concurrent experiments limit reached');
    }

    const experimentId = this.generateExperimentId(experimentType);
    logDebug(`üí• Starting chaos experiment: ${experimentId} (${experimentType})`);

    const experiment = {
      id: experimentId,
      type: experimentType,
      config: { ...this.config.experiments[experimentType], ...customConfig },
      startTime: new Date().toISOString(),
      status: 'running',
      phases: [],
      metrics: {},
      errors: []
    };

    this.activeExperiments.set(experimentId, experiment);
    
    try {
      // Pre-experiment safety checks
      await this.performSafetyChecks(experiment);
      
      // Execute experiment phases
      await this.executeExperimentPhase(experiment, _'preparation', async _() => {
        await this.prepareExperiment(experiment);
      });

      await this.executeExperimentPhase(experiment, _'fault-injection', async _() => {
        await this.injectFault(experiment);
      });

      await this.executeExperimentPhase(experiment, _'observation', async _() => {
        await this.observeSystemBehavior(experiment);
      });

      await this.executeExperimentPhase(experiment, _'recovery', async _() => {
        await this.recoverFromFault(experiment);
      });

      await this.executeExperimentPhase(experiment, _'validation', async _() => {
        await this.validateRecovery(experiment);
      });

      experiment.status = 'completed';
      experiment.endTime = new Date().toISOString();
      experiment.duration = new Date(experiment.endTime) - new Date(experiment.startTime);

      logDebug(`‚úÖ Chaos experiment ${experimentId} completed successfully`);
      this.emit('experimentCompleted', experiment);

    } catch (error) {
      experiment.status = 'failed';
      experiment.error = error.message;
      experiment.endTime = new Date().toISOString();
      
      logError(`‚ùå Chaos experiment ${experimentId} failed: ${error.message}`);
      
      // Emergency recovery
      await this.emergencyRecovery(experiment);
      
      this.emit('experimentFailed', experiment);
      throw error;

    } finally {
      this.activeExperiments.delete(experimentId);
      this.experimentHistory.push(experiment);
      
      // Save experiment record
      await this.saveExperimentRecord(experiment);
      
      // Cooldown period
      setTimeout(_() => {
        this.emit('cooldownComplete');
      }, this.config.safety.cooldownPeriod);
    }

    return experiment;
  }

  async executeExperimentPhase(experiment, phaseName, phaseFunction) {
    const phase = {
      name: phaseName,
      startTime: new Date().toISOString(),
      status: 'running'
    };

    experiment.phases.push(phase);
    logDebug(`üîÑ Executing phase: ${phaseName}`);

    try {
      const startTime = Date.now();
      await phaseFunction();
      
      phase.endTime = new Date().toISOString();
      phase.duration = Date.now() - startTime;
      phase.status = 'completed';
      
      logDebug(`‚úÖ Phase ${phaseName} completed in ${phase.duration}ms`);
    } catch (error) {
      phase.endTime = new Date().toISOString();
      phase.status = 'failed';
      phase.error = error.message;
      
      logError(`‚ùå Phase ${phaseName} failed: ${error.message}`);
      throw error;
    }
  }

  async performSafetyChecks(experiment) {
    logDebug('üõ°Ô∏è Performing safety checks...');
    
    // Check system health
    const systemHealth = await this.checkSystemHealth();
    if (!systemHealth.healthy) {
      throw new Error(`System not healthy enough for chaos experiment: ${systemHealth.issues.join(', ')}`);
    }

    // Check resource availability
    const resourceCheck = await this.checkResourceAvailability();
    if (!resourceCheck.sufficient) {
      throw new Error(`Insufficient resources for chaos experiment: ${resourceCheck.issues.join(', ')}`);
    }

    // Validate experiment duration
    if (experiment.config.duration > this.config.safety.maxExperimentDuration) {
      throw new Error(`Experiment duration exceeds safety limit`);
    }

    logDebug('‚úÖ Safety checks passed');
  }

  async prepareExperiment(experiment) {
    logDebug('üîß Preparing chaos experiment...');
    
    // Create experiment snapshots
    experiment.preExperimentSnapshot = {
      metrics: await this.captureSystemMetrics(),
      services: await this.captureServiceStates(),
      resources: await this.captureResourceStates()
    };

    // Setup experiment-specific monitoring
    if (experiment.type === 'networkLatency') {
      await this.setupNetworkMonitoring(experiment);
    } else if (experiment.type === 'resourceExhaustion') {
      await this.setupResourceMonitoring(experiment);
    }

    logDebug('‚úÖ Experiment preparation completed');
  }

  async injectFault(experiment) {
    logDebug(`üíâ Injecting fault: ${experiment.type}`);
    
    const chaosController = this.chaosTools.get(this.getChaosControllerType(experiment.type));
    if (!chaosController) {
      throw new Error(`No chaos controller found for experiment type: ${experiment.type}`);
    }

    experiment.faultInjection = await chaosController.injectFault(experiment);
    
    logDebug('‚úÖ Fault injection completed');
  }

  async observeSystemBehavior(experiment) {
    logDebug('üëÅÔ∏è Observing system behavior under chaos...');
    
    const observationDuration = experiment.config.duration || 30000;
    const metricsInterval = 5000; // Collect metrics every 5 seconds
    const observations = [];
    
    const startTime = Date.now();
    while ((Date.now() - startTime) < observationDuration) {
      const metrics = await this.captureSystemMetrics();
      observations.push({
        timestamp: new Date().toISOString(),
        metrics
      });
      
      // Check for emergency conditions
      await this.checkEmergencyConditions();
      
      await this.sleep(metricsInterval);
    }

    experiment.observations = observations;
    experiment.behaviorAnalysis = await this.analyzeSystemBehavior(observations);
    
    logDebug('‚úÖ System behavior observation completed');
  }

  async recoverFromFault(experiment) {
    logDebug('üîÑ Recovering from injected fault...');
    
    const chaosController = this.chaosTools.get(this.getChaosControllerType(experiment.type));
    if (chaosController && experiment.faultInjection) {
      await chaosController.recoverFault(experiment.faultInjection);
    }

    // Additional recovery steps
    if (this.config.recovery.serviceRestart) {
      await this.restartAffectedServices(experiment);
    }

    if (this.config.recovery.circuitBreakerReset) {
      await this.resetCircuitBreakers();
    }

    logDebug('‚úÖ Fault recovery completed');
  }

  async validateRecovery(experiment) {
    logDebug('üîç Validating system recovery...');
    
    // Wait for system stabilization
    await this.sleep(10000);
    
    const postRecoveryMetrics = await this.captureSystemMetrics();
    const recoveryValidation = await this.validateSystemRecovery(
      experiment.preExperimentSnapshot.metrics,
      postRecoveryMetrics
    );

    experiment.recoveryValidation = recoveryValidation;
    
    if (!recoveryValidation.recovered) {
      throw new Error(`System did not recover properly: ${recoveryValidation.issues.join(', ')}`);
    }

    logDebug('‚úÖ Recovery validation passed');
  }

  // Chaos controller implementations
  getChaosControllerType(experimentType) {
    const typeMapping = {
      'networkLatency': 'network',
      'networkPartition': 'network',
      'resourceExhaustion': 'resource',
      'serviceFailure': 'service',
      'databaseFailure': 'database',
      'fileSystemChaos': 'filesystem'
    };
    
    return typeMapping[experimentType] || 'generic';
  }

  // System monitoring and metrics
  async captureSystemMetrics() {
    const metrics = {
      timestamp: new Date().toISOString(),
      responseTime: await this.measureResponseTime(),
      throughput: await this.measureThroughput(),
      errorRate: await this.measureErrorRate(),
      resourceUsage: await this.measureResourceUsage(),
      serviceHealth: await this.checkServiceHealth()
    };

    return metrics;
  }

  async measureResponseTime() {
    const endpoints = [
      'http://localhost:3000/api/health',
      'http://localhost:5000/api/health',
      'http://localhost:3000/'
    ];

    const measurements = await Promise.allSettled(
      endpoints.map(async (endpoint) => {
        const start = Date.now();
        try {
          const response = await fetch(endpoint, { timeout: 10000 });
          return Date.now() - start;
        } catch (error) {
          return null;
        }
      })
    );

    const validMeasurements = measurements
      .filter(result => result.status === 'fulfilled' && result.value !== null)
      .map(result => result.value);

    return validMeasurements.length > 0 
      ? validMeasurements.reduce((a, _b) => a + b, 0) / validMeasurements.length
      : null;
  }

  async measureThroughput() {
    // Simplified throughput measurement
    try {
      const start = Date.now();
      const requests = Array(10).fill().map(() => 
        fetch('http://localhost:3000/api/health', { timeout: 5000 })
      );
      
      await Promise.allSettled(requests);
      const duration = Date.now() - start;
      
      return requests.length / (duration / 1000); // requests per second
    } catch (error) {
      return 0;
    }
  }

  async measureErrorRate() {
    const endpoints = [
      'http://localhost:3000/api/health',
      'http://localhost:5000/api/health'
    ];

    const results = await Promise.allSettled(
      endpoints.map(endpoint => fetch(endpoint, { timeout: 5000 }))
    );

    const totalRequests = results.length;
    const errorRequests = results.filter(result => 
      result.status === 'rejected' || 
      (result.status === 'fulfilled' && !result.value.ok)
    ).length;

    return totalRequests > 0 ? (errorRequests / totalRequests) * 100 : 0;
  }

  async measureResourceUsage() {
    const memUsage = process.memoryUsage();
    const cpuUsage = process.cpuUsage();
    
    return {
      memory: {
        used: memUsage.rss,
        heap: memUsage.heapUsed,
        external: memUsage.external,
        percentage: (memUsage.rss / os.totalmem()) * 100
      },
      cpu: {
        user: cpuUsage.user,
        system: cpuUsage.system,
        percentage: this.calculateCpuPercentage(cpuUsage)
      },
      uptime: process.uptime()
    };
  }

  calculateCpuPercentage(cpuUsage) {
    // Simplified CPU percentage calculation
    const totalUsage = cpuUsage.user + cpuUsage.system;
    const uptime = process.uptime() * 1000000; // Convert to microseconds
    return (totalUsage / uptime) * 100;
  }

  async checkServiceHealth() {
    const services = [
      { name: 'frontend', url: 'http://localhost:3000/api/health' },
      { name: 'backend', url: 'http://localhost:5000/api/health' }
    ];

    const healthChecks = await Promise.allSettled(
      services.map(async (service) => {
        try {
          const response = await fetch(service.url, { timeout: 5000 });
          return {
            name: service.name,
            healthy: response.ok,
            status: response.status,
            responseTime: response.headers.get('x-response-time')
          };
        } catch (error) {
          return {
            name: service.name,
            healthy: false,
            error: error.message
          };
        }
      })
    );

    return healthChecks.map((check, _index) => ({
      ...services[index],
      ...check.value
    }));
  }

  async checkSystemHealth() {
    const health = await this.checkServiceHealth();
    const resources = await this.measureResourceUsage();
    
    const issues = [];
    
    // Check service health
    const unhealthyServices = health.filter(service => !service.healthy);
    if (unhealthyServices.length > 0) {
      issues.push(`Unhealthy services: ${unhealthyServices.map(s => s.name).join(', ')}`);
    }

    // Check resource usage
    if (resources.memory.percentage > 90) {
      issues.push('High memory usage');
    }
    if (resources.cpu.percentage > 90) {
      issues.push('High CPU usage');
    }

    return {
      healthy: issues.length === 0,
      issues,
      services: health,
      resources
    };
  }

  async checkResourceAvailability() {
    const resources = await this.measureResourceUsage();
    const issues = [];

    if (resources.memory.percentage > 80) {
      issues.push('Memory usage too high for safe chaos testing');
    }
    if (resources.cpu.percentage > 80) {
      issues.push('CPU usage too high for safe chaos testing');
    }

    return {
      sufficient: issues.length === 0,
      issues,
      resources
    };
  }

  async analyzeSystemBehavior(observations) {
    const analysis = {
      resilience: 'unknown',
      degradationDetected: false,
      recoveryTime: null,
      performanceImpact: {},
      recommendations: []
    };

    if (observations.length === 0) {
      return analysis;
    }

    // Analyze response time trends
    const responseTimes = observations.map(obs => obs.metrics.responseTime).filter(rt => rt !== null);
    if (responseTimes.length > 0) {
      const baselineResponseTime = this.systemBaseline.metrics.responseTime;
      const avgResponseTime = responseTimes.reduce((a, _b) => a + b, 0) / responseTimes.length;
      
      analysis.performanceImpact.responseTime = {
        baseline: baselineResponseTime,
        average: avgResponseTime,
        degradation: ((avgResponseTime - baselineResponseTime) / baselineResponseTime) * 100
      };
    }

    // Analyze error rates
    const errorRates = observations.map(obs => obs.metrics.errorRate);
    const avgErrorRate = errorRates.reduce((a, _b) => a + b, 0) / errorRates.length;
    const baselineErrorRate = this.systemBaseline.metrics.errorRate;
    
    analysis.performanceImpact.errorRate = {
      baseline: baselineErrorRate,
      average: avgErrorRate,
      increase: avgErrorRate - baselineErrorRate
    };

    // Determine overall resilience
    if (analysis.performanceImpact.errorRate?.increase > 50) {
      analysis.resilience = 'poor';
      analysis.recommendations.push('Implement better error handling and retry mechanisms');
    } else if (analysis.performanceImpact.responseTime?.degradation > 200) {
      analysis.resilience = 'moderate';
      analysis.recommendations.push('Consider performance optimizations under stress');
    } else {
      analysis.resilience = 'good';
    }

    return analysis;
  }

  async validateSystemRecovery(preExperimentMetrics, postRecoveryMetrics) {
    const validation = {
      recovered: true,
      issues: [],
      metrics: {}
    };

    // Compare response times
    if (postRecoveryMetrics.responseTime && preExperimentMetrics.responseTime) {
      const responseTimeDelta = Math.abs(
        postRecoveryMetrics.responseTime - preExperimentMetrics.responseTime
      );
      
      validation.metrics.responseTime = {
        before: preExperimentMetrics.responseTime,
        after: postRecoveryMetrics.responseTime,
        delta: responseTimeDelta,
        recovered: responseTimeDelta < (preExperimentMetrics.responseTime * 0.5) // Within 50%
      };
      
      if (!validation.metrics.responseTime.recovered) {
        validation.recovered = false;
        validation.issues.push('Response time not recovered to baseline');
      }
    }

    // Compare error rates
    const errorRateDelta = Math.abs(
      postRecoveryMetrics.errorRate - preExperimentMetrics.errorRate
    );
    
    validation.metrics.errorRate = {
      before: preExperimentMetrics.errorRate,
      after: postRecoveryMetrics.errorRate,
      delta: errorRateDelta,
      recovered: errorRateDelta < 10 // Within 10% error rate
    };

    if (!validation.metrics.errorRate.recovered) {
      validation.recovered = false;
      validation.issues.push('Error rate not recovered to baseline');
    }

    return validation;
  }

  // Emergency procedures
  async checkEmergencyConditions() {
    const currentMetrics = await this.captureSystemMetrics();
    
    // Check for critical conditions
    if (currentMetrics.errorRate > 80) {
      await this.triggerEmergencyStop('Critical error rate exceeded');
    }
    
    if (currentMetrics.resourceUsage.memory.percentage > 95) {
      await this.triggerEmergencyStop('Critical memory usage');
    }
    
    const unhealthyServices = currentMetrics.serviceHealth.filter(s => !s.healthy);
    if (unhealthyServices.length >= 2) {
      await this.triggerEmergencyStop('Multiple services down');
    }
  }

  async triggerEmergencyStop(reason) {
    if (this.emergencyStopActive) return;
    
    logError(`üö® EMERGENCY STOP TRIGGERED: ${reason}`);
    this.emergencyStopActive = true;
    
    // Stop all active experiments
    for (const [experimentId, experiment] of this.activeExperiments) {
      logDebug(`üõë Emergency stopping experiment: ${experimentId}`);
      await this.emergencyRecovery(experiment);
    }
    
    this.emit('emergencyStop', { reason, timestamp: new Date().toISOString() });
  }

  async emergencyRecovery(experiment) {
    logDebug(`üöë Emergency recovery for experiment: ${experiment.id}`);
    
    try {
      // Attempt to recover from any injected faults
      const chaosController = this.chaosTools.get(this.getChaosControllerType(experiment.type));
      if (chaosController && experiment.faultInjection) {
        await chaosController.emergencyRecover(experiment.faultInjection);
      }
      
      // Restart services if needed
      await this.restartAffectedServices(experiment);
      
      // Reset circuit breakers
      await this.resetCircuitBreakers();
      
      logDebug(`‚úÖ Emergency recovery completed for: ${experiment.id}`);
      
    } catch (error) {
      logError(`‚ùå Emergency recovery failed for ${experiment.id}: ${error.message}`);
    }
  }

  async restartAffectedServices(experiment) {
    // This would typically restart services based on experiment type
    logDebug('üîÑ Restarting affected services...');
    
    if (experiment.type === 'serviceFailure') {
      // Restart specific services affected by the experiment
      // Implementation would depend on your service management system
    }
  }

  async resetCircuitBreakers() {
    // Reset circuit breakers in the application
    logDebug('üîÑ Resetting circuit breakers...');
    
    try {
      // This would call your application's circuit breaker reset endpoint
      await fetch('http://localhost:5000/api/admin/circuit-breakers/reset', {
        method: 'POST',
        timeout: 5000
      });
    } catch (error) {
      logWarn(`Circuit breaker reset failed: ${error.message}`);
    }
  }

  // Utility methods
  generateExperimentId(type) {
    const timestamp = new Date().toISOString().replace(/[:.]/g, '');
    const random = Math.random().toString(36).substr(2, 6);
    return `chaos_${type}_${timestamp}_${random}`;
  }

  async sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  async saveExperimentRecord(experiment) {
    const recordPath = path.join(
      process.cwd(),
      'tests/chaos/experiments',
      `${experiment.id}.json`
    );
    
    fs.writeFileSync(recordPath, JSON.stringify(experiment, null, 2));
  }

  // Integration methods
  async integrateWithAutonomousSystem() {
    logDebug('üîó Integrating chaos engineering with autonomous system...');
    
    const chaosScenarios = this.generateChaosTestScenarios();
    
    fs.writeFileSync(
      path.join(process.cwd(), 'tests/autonomous/chaos-test-scenarios.json'),
      JSON.stringify(chaosScenarios, null, 2)
    );

    logDebug(`üí• Generated ${chaosScenarios.length} chaos test scenarios`);
    return chaosScenarios;
  }

  generateChaosTestScenarios() {
    const scenarios = [];
    
    Object.keys(this.config.experiments).forEach(experimentType => {
      if (this.config.experiments[experimentType].enabled) {
        scenarios.push({
          name: `CHAOS_EXPERIMENT_${experimentType.toUpperCase()}`,
          type: 'chaos',
          experimentType,
          priority: 'medium',
          timeout: 300000, // 5 minutes
          retries: 1,
          execution: async () => {
            return await this.runChaosExperiment(experimentType);
          }
        });
      }
    });

    return scenarios;
  }

  // Public API methods
  getChaosStatus() {
    return {
      initialized: this.chaosTools.size > 0,
      activeExperiments: this.activeExperiments.size,
      totalExperiments: this.experimentHistory.length,
      emergencyStopActive: this.emergencyStopActive,
      lastExperiment: this.experimentHistory[this.experimentHistory.length - 1]?.id
    };
  }

  async runAllChaosExperiments() {
    logDebug('üí• Running all enabled chaos experiments...');
    
    const results = [];
    
    for (const [experimentType, config] of Object.entries(this.config.experiments)) {
      if (config.enabled) {
        try {
          const result = await this.runChaosExperiment(experimentType);
          results.push(result);
          
          // Wait for cooldown between experiments
          await this.sleep(this.config.safety.cooldownPeriod);
          
        } catch (error) {
          results.push({
            type: experimentType,
            status: 'error',
            error: error.message
          });
        }
      }
    }

    return results;
  }

  resetEmergencyStop() {
    this.emergencyStopActive = false;
    logDebug('üîÑ Emergency stop reset - chaos experiments re-enabled');
  }
}

// Chaos controller classes (simplified implementations)
class NetworkChaosController {
  async injectFault(experiment) {
    logDebug(`üåê Injecting network fault: ${experiment.type}`);
    
    if (experiment.type === 'networkLatency') {
      return this.injectNetworkLatency(experiment.config);
    } else if (experiment.type === 'networkPartition') {
      return this.injectNetworkPartition(experiment.config);
    }
  }

  async injectNetworkLatency(config) {
    // Platform-specific network latency injection
    const platform = os.platform();
    
    if (platform === 'linux') {
      // Use tc (traffic control) on Linux
      const latency = Math.floor(Math.random() * (config.maxLatency - config.minLatency)) + config.minLatency;
      await execAsync(`tc qdisc add dev lo root netem delay ${latency}ms`);
      return { type: 'networkLatency', latency, interface: 'lo' };
    } else {
      // Simplified simulation for other platforms
      logDebug(`Simulating ${config.maxLatency}ms network latency`);
      return { type: 'networkLatency', latency: config.maxLatency, simulated: true };
    }
  }

  async injectNetworkPartition(config) {
    // Simulate network partition by blocking specific ports
    logDebug('Simulating network partition');
    return { type: 'networkPartition', ports: config.targetPorts, simulated: true };
  }

  async recoverFault(faultInjection) {
    logDebug(`üîÑ Recovering network fault: ${faultInjection.type}`);
    
    if (faultInjection.type === 'networkLatency' && !faultInjection.simulated) {
      await execAsync('tc qdisc del dev lo root').catch(_() => {
        // Ignore errors - rule might not exist
      });
    }
  }

  async emergencyRecover(faultInjection) {
    await this.recoverFault(faultInjection);
  }
}

class ResourceChaosController {
  async injectFault(experiment) {
    logDebug(`üíª Injecting resource fault: ${experiment.type}`);
    return { type: 'resourceExhaustion', simulated: true };
  }

  async recoverFault(faultInjection) {
    logDebug(`üîÑ Recovering resource fault: ${faultInjection.type}`);
  }

  async emergencyRecover(faultInjection) {
    await this.recoverFault(faultInjection);
  }
}

class ServiceChaosController {
  async injectFault(experiment) {
    logDebug(`‚öôÔ∏è Injecting service fault: ${experiment.type}`);
    return { type: 'serviceFailure', simulated: true };
  }

  async recoverFault(faultInjection) {
    logDebug(`üîÑ Recovering service fault: ${faultInjection.type}`);
  }

  async emergencyRecover(faultInjection) {
    await this.recoverFault(faultInjection);
  }
}

class DatabaseChaosController {
  async injectFault(experiment) {
    logDebug(`üóÑÔ∏è Injecting database fault: ${experiment.type}`);
    return { type: 'databaseFailure', simulated: true };
  }

  async recoverFault(faultInjection) {
    logDebug(`üîÑ Recovering database fault: ${faultInjection.type}`);
  }

  async emergencyRecover(faultInjection) {
    await this.recoverFault(faultInjection);
  }
}

class FileSystemChaosController {
  async injectFault(experiment) {
    logDebug(`üìÅ Injecting filesystem fault: ${experiment.type}`);
    return { type: 'fileSystemChaos', simulated: true };
  }

  async recoverFault(faultInjection) {
    logDebug(`üîÑ Recovering filesystem fault: ${faultInjection.type}`);
  }

  async emergencyRecover(faultInjection) {
    await this.recoverFault(faultInjection);
  }
}

export default ChaosEngineeringEngine;
export { ChaosEngineeringEngine };